{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа №8\n"
      ],
      "metadata": {
        "id": "FjDFTlulCkO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Выбор начальных условий\n",
        "a.\tВыбрать набор данных для задачи классификации и обосновать его выбор.\n",
        "Датасет - https://www.kaggle.com/datasets/ronakgohil/license-plate-dataset\n",
        "\n",
        "Данный датасет с изображениями автомобильных номерных знаков актуален для задач автоматического распознавания номерных знаков, что важно для систем контроля транспорта, безопасности на дорогах и управления парковками.\n",
        "\n",
        "b.\tВыбрать метрики качества и обосновать их выбор.\n",
        "\n",
        "\n",
        "* mAP\\@0.5 — измеряет, насколько модель правильно находит номера с допуском локализации (IoU=0.5).\n",
        "* mAP@\\[0.5:0.95] — средний показатель качества детекции при разных уровнях точности локализации.\n",
        "* Precision — доля правильных номеров среди всех найденных. Важно избегать ложных срабатываний.\n",
        "* Recall — доля найденных номеров из всех реально присутствующих. Важно не пропускать номера.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7V-ly_n_C09d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.\tСоздание бейзлайна и оценка качества\n"
      ],
      "metadata": {
        "id": "PQxtNd8KDsN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle --quiet\n",
        "!pip install ultralytics --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TjUAAIOH6_1",
        "outputId": "ae8b4496-f991-4d5d-9241-d83b37e0b61c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m474.6/664.8 MB\u001b[0m \u001b[31m162.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
            "    unknown package:\n",
            "        Expected sha256 165764f44ef8c61fcdfdfdbe769d687e06374059fbb388b6c89ecb0e28793a6f\n",
            "             Got        de172a60ff8fce5c122f06be9b7da4285bb4a2a139f8f77c7b74e9d5e67fee18\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFdLQSTLNxH",
        "outputId": "88348734-1038-4e87-c139-4635766d66bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FfGZwfFbk9U",
        "outputId": "94604a1b-50b6-44d5-e08e-fb7dc2bcfe45"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.14.3 torchmetrics-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим датасет"
      ],
      "metadata": {
        "id": "RR7ErP34MCqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "y_Aa8rO_NsEn",
        "outputId": "8e3407c4-d87f-4e81-8cf8-6b3084f7242d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a56ef0d7-7881-4392-8ea6-4a111203e075\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a56ef0d7-7881-4392-8ea6-4a111203e075\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ketrum\",\"key\":\"4f7898fa6bc17d128bca596c0fb2323f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "os.rename('kaggle.json', '/root/.kaggle/kaggle.json')"
      ],
      "metadata": {
        "id": "QFodT8LkNuZm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d ronakgohil/license-plate-dataset\n",
        "!unzip -q license-plate-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tThwzqXNwa-",
        "outputId": "22dbd731-622b-4f87-8f87-3614eacf4e92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/ronakgohil/license-plate-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading license-plate-dataset.zip to /content\n",
            " 77% 114M/147M [00:00<00:00, 1.19GB/s]\n",
            "100% 147M/147M [00:00<00:00, 821MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a.\tОбучить модель для выбранного набора данных"
      ],
      "metadata": {
        "id": "VWdiJ5TfEAyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "yolo = YOLO('yolo11n.pt')\n",
        "yaml_path = 'archive/dataset.yaml'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cTejExpJEts",
        "outputId": "2046e842-66a9-4da3-cb0f-d575d2954e51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 349MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_yolo = yolo.train(\n",
        "    data=yaml_path,\n",
        "    epochs=5,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "    name='license-plate'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izskAPA6Jr6d",
        "outputId": "6a5b0a9e-0105-4f87-f156-a9b9d4efea5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=archive/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=license-plate5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/license-plate5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 97.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 13.5±4.9 MB/s, size: 24.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/archive/labels/train... 1526 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1526/1526 [00:01<00:00, 814.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/archive/images/train/car-wbs-MH03AR5549_00000.jpg: corrupt JPEG restored and saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/archive/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 48.8±32.5 MB/s, size: 162.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/archive/labels/val... 169 images, 0 backgrounds, 0 corrupt: 100%|██████████| 169/169 [00:00<00:00, 354.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/archive/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/license-plate5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/license-plate5\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5     0.637G      1.652      2.326      1.158         39        224: 100%|██████████| 48/48 [00:12<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169    0.00555      0.959     0.0306     0.0155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5     0.793G       1.23     0.9754      0.959         42        224: 100%|██████████| 48/48 [00:11<00:00,  4.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169       0.97      0.764      0.928      0.621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5     0.809G      1.108      0.829     0.9394         32        224: 100%|██████████| 48/48 [00:10<00:00,  4.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.966      0.852      0.952      0.672\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5     0.822G      1.066     0.7628     0.9355         53        224: 100%|██████████| 48/48 [00:10<00:00,  4.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.916      0.923       0.97      0.661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5     0.838G     0.9895     0.6903      0.907         39        224: 100%|██████████| 48/48 [00:10<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.987      0.929      0.981       0.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.018 hours.\n",
            "Optimizer stripped from runs/detect/license-plate5/weights/last.pt, 5.4MB\n",
            "Optimizer stripped from runs/detect/license-plate5/weights/best.pt, 5.4MB\n",
            "\n",
            "Validating runs/detect/license-plate5/weights/best.pt...\n",
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.987      0.929      0.981      0.739\n",
            "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/license-plate5\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b.\tОценить качество модели по выбранным метрикам на выбранном наборе данных\n"
      ],
      "metadata": {
        "id": "yg3ONjlXEF7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_val_base = yolo.val(data=yaml_path, iou=0.5)\n",
        "print(\"Метрики:\")\n",
        "print(\"mAP@0.5:\", results_val_base.box.map50)\n",
        "print(\"mAP@0.5:0.95:\", results_val_base.box.map)\n",
        "print(\"Precision:\", results_val_base.box.p)\n",
        "print(\"Recall:\", results_val_base.box.r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7129L1SJyk1",
        "outputId": "a89a474f-79e1-4265-afdb-0b71a33ce547"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2719.8±995.2 MB/s, size: 204.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/archive/labels/val.cache... 169 images, 0 backgrounds, 0 corrupt: 100%|██████████| 169/169 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.987      0.929      0.981      0.739\n",
            "Speed: 0.5ms preprocess, 4.5ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/license-plate52\u001b[0m\n",
            "Метрики:\n",
            "mAP@0.5: 0.98132428952884\n",
            "mAP@0.5:0.95: 0.7391155052513505\n",
            "Precision: [    0.98742]\n",
            "Recall: [    0.92866]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.\tУлучшение бейзлайна"
      ],
      "metadata": {
        "id": "PTT1TMsMD0Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Сформулировать гипотезы (аугментации данных, подбор моделей, подбор гиперпараметров и т.д.)"
      ],
      "metadata": {
        "id": "nD2RyBefEMZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим следующие гипотезы; 1) увеличение количества эпох 2) аугментация 3) увеличение скорости обучения 4) использование модели yolo11s"
      ],
      "metadata": {
        "id": "jl-qHVJRRYrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Проверить гипотезы"
      ],
      "metadata": {
        "id": "H-x2RsqQEMOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) увеличение количества эпох"
      ],
      "metadata": {
        "id": "en7qqJDUSWSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_yolo = yolo.train(\n",
        "    data=yaml_path,\n",
        "    epochs=10,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "    name='license-plate'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pAgC1jMOJ_P",
        "outputId": "6b1dea00-820e-4341-e0f2-52cfd693bc12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=archive/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=license-plate6, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/license-plate6, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 94/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 940.2±503.3 MB/s, size: 24.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/archive/labels/train.cache... 1526 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1526/1526 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/archive/images/train/car-wbs-MH03AR5549_00000.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 396.9±233.0 MB/s, size: 162.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/archive/labels/val.cache... 169 images, 0 backgrounds, 0 corrupt: 100%|██████████| 169/169 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/license-plate6/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/license-plate6\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10     0.664G      1.655      1.197      1.175         22        224: 100%|██████████| 48/48 [00:11<00:00,  4.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.932      0.811      0.905      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10     0.729G      1.125     0.8324     0.9641         22        224: 100%|██████████| 48/48 [00:11<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.968      0.892       0.96      0.635\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10     0.729G      1.039     0.7386     0.9362         22        224: 100%|██████████| 48/48 [00:09<00:00,  5.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.965      0.893      0.954      0.676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10     0.729G      1.001      0.682     0.9242         22        224: 100%|██████████| 48/48 [00:09<00:00,  5.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.964      0.961      0.979      0.731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10     0.729G     0.9639     0.6311     0.9091         22        224: 100%|██████████| 48/48 [00:09<00:00,  5.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.988      0.945      0.981      0.742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10     0.736G     0.9267     0.5922     0.8901         22        224: 100%|██████████| 48/48 [00:10<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.994      0.963      0.985      0.768\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10     0.736G     0.9032     0.5813     0.8811         22        224: 100%|██████████| 48/48 [00:10<00:00,  4.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.982      0.958      0.984      0.736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10     0.736G     0.8798     0.5443     0.8724         22        224: 100%|██████████| 48/48 [00:10<00:00,  4.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.993      0.976      0.987      0.768\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10     0.752G     0.8351      0.513     0.8688         22        224: 100%|██████████| 48/48 [00:12<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.982      0.964      0.987      0.782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10     0.752G     0.8216     0.5085     0.8637         22        224: 100%|██████████| 48/48 [00:11<00:00,  4.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.988       0.97      0.987      0.779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.034 hours.\n",
            "Optimizer stripped from runs/detect/license-plate6/weights/last.pt, 5.4MB\n",
            "Optimizer stripped from runs/detect/license-plate6/weights/best.pt, 5.4MB\n",
            "\n",
            "Validating runs/detect/license-plate6/weights/best.pt...\n",
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.982      0.964      0.987       0.78\n",
            "Speed: 0.0ms preprocess, 0.9ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/license-plate6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) аугментация"
      ],
      "metadata": {
        "id": "kQZdO3A4SYNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_yolo = yolo.train(\n",
        "    data=yaml_path,\n",
        "    epochs=5,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "    augment=True,\n",
        "    name='license-plate'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nhrChQfOMdX",
        "outputId": "ed5f499a-2f07-4bab-b183-f9d7ca5668d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=archive/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=license-plate7, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/license-plate7, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 499/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 703.7±269.7 MB/s, size: 24.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/archive/labels/train.cache... 1526 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1526/1526 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/archive/images/train/car-wbs-MH03AR5549_00000.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 454.2±118.6 MB/s, size: 162.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/archive/labels/val.cache... 169 images, 0 backgrounds, 0 corrupt: 100%|██████████| 169/169 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/license-plate7/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/license-plate7\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5     0.676G     0.9734     0.6743     0.9023         39        224: 100%|██████████| 48/48 [00:11<00:00,  4.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169       0.97      0.961      0.983      0.751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5     0.787G     0.9756     0.6041     0.8957         42        224: 100%|██████████| 48/48 [00:11<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.981      0.959      0.983      0.754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5     0.787G     0.9417     0.5857     0.8943         32        224: 100%|██████████| 48/48 [00:11<00:00,  4.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.984      0.959      0.983      0.766\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5     0.787G     0.9322     0.5762     0.8975         53        224: 100%|██████████| 48/48 [00:12<00:00,  3.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.962       0.97      0.986      0.747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5     0.789G     0.8912     0.5495      0.884         39        224: 100%|██████████| 48/48 [00:11<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.968      0.976      0.985      0.766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.018 hours.\n",
            "Optimizer stripped from runs/detect/license-plate7/weights/last.pt, 5.4MB\n",
            "Optimizer stripped from runs/detect/license-plate7/weights/best.pt, 5.4MB\n",
            "\n",
            "Validating runs/detect/license-plate7/weights/best.pt...\n",
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.946      0.942      0.981      0.754\n",
            "Speed: 0.1ms preprocess, 7.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/license-plate7\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3) увеличение скорости обучения"
      ],
      "metadata": {
        "id": "gfa1IS93PA5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_yolo = yolo.train(\n",
        "    data=yaml_path,\n",
        "    lr0=0.01,\n",
        "    epochs=5,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "\n",
        "    name='license-plate'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTZr0YqkORyG",
        "outputId": "0f1e3388-f28d-4c7c-baf6-239f58d9429f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=archive/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=license-plate9, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/license-plate9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 499/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 873.3±393.4 MB/s, size: 24.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/archive/labels/train.cache... 1526 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1526/1526 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/archive/images/train/car-wbs-MH03AR5549_00000.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 518.5±100.3 MB/s, size: 162.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/archive/labels/val.cache... 169 images, 0 backgrounds, 0 corrupt: 100%|██████████| 169/169 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/license-plate9/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/license-plate9\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5     0.809G      1.057     0.9087     0.9143         39        224: 100%|██████████| 48/48 [00:11<00:00,  4.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.952      0.941      0.979      0.776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5     0.809G     0.8954     0.5434     0.8769         42        224: 100%|██████████| 48/48 [00:11<00:00,  4.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.937      0.964      0.982      0.771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5     0.809G     0.8628     0.5224     0.8735         32        224: 100%|██████████| 48/48 [00:10<00:00,  4.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169       0.97      0.971      0.986      0.772\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5     0.816G     0.8546     0.5172     0.8785         53        224: 100%|██████████| 48/48 [00:10<00:00,  4.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.979       0.97       0.99      0.767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5      0.83G     0.8448     0.5114     0.8704         39        224: 100%|██████████| 48/48 [00:11<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.971      0.981      0.989      0.785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.017 hours.\n",
            "Optimizer stripped from runs/detect/license-plate9/weights/last.pt, 5.4MB\n",
            "Optimizer stripped from runs/detect/license-plate9/weights/best.pt, 5.4MB\n",
            "\n",
            "Validating runs/detect/license-plate9/weights/best.pt...\n",
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.968      0.947      0.986      0.774\n",
            "Speed: 0.1ms preprocess, 2.8ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/license-plate9\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4) использование модели yolo11s"
      ],
      "metadata": {
        "id": "xIkva5mLORn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo = YOLO('yolo11s.pt')\n",
        "res_yolo = yolo.train(\n",
        "    data=yaml_path,\n",
        "    epochs=5,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "    name='license-plate'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVp2LomhPcnW",
        "outputId": "ea76e0f5-cdcf-4407-a19b-9a397d0812ab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt to 'yolo11s.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18.4M/18.4M [00:00<00:00, 437MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=archive/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=license-plate10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/license-plate10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
            " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
            " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 23        [16, 19, 22]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "YOLO11s summary: 181 layers, 9,428,179 parameters, 9,428,163 gradients, 21.5 GFLOPs\n",
            "\n",
            "Transferred 493/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 998.6±467.6 MB/s, size: 24.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/archive/labels/train.cache... 1526 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1526/1526 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/archive/images/train/car-wbs-MH03AR5549_00000.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 489.6±92.6 MB/s, size: 162.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/archive/labels/val.cache... 169 images, 0 backgrounds, 0 corrupt: 100%|██████████| 169/169 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/license-plate10/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/license-plate10\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5      1.14G      1.674       2.18      1.201         39        224: 100%|██████████| 48/48 [00:12<00:00,  3.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169     0.0754      0.609     0.0677     0.0413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5      1.25G       1.14       0.74     0.9668         42        224: 100%|██████████| 48/48 [00:10<00:00,  4.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.941      0.811      0.883      0.582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5      1.29G      1.052      0.658     0.9483         32        224: 100%|██████████| 48/48 [00:10<00:00,  4.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.316      0.893      0.311       0.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5      1.33G       1.01     0.6189     0.9433         53        224: 100%|██████████| 48/48 [00:10<00:00,  4.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.866      0.846      0.862      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5      1.37G     0.9178     0.5574     0.9085         39        224: 100%|██████████| 48/48 [00:10<00:00,  4.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.979       0.97      0.991      0.768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.018 hours.\n",
            "Optimizer stripped from runs/detect/license-plate10/weights/last.pt, 19.1MB\n",
            "Optimizer stripped from runs/detect/license-plate10/weights/best.pt, 19.1MB\n",
            "\n",
            "Validating runs/detect/license-plate10/weights/best.pt...\n",
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.979       0.97      0.991       0.77\n",
            "Speed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 4.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/license-plate10\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  c. Сформировать улучшенный бейзлайн по результатам проверки гипотез"
      ],
      "metadata": {
        "id": "4--L2m26EMFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В результате проверки гипотез, все гипотезы показали улучшения. Улучшенный бейзлайн включает в себя: 1) увеличение количества эпох 2) аугментация  3) увеличение скорости обучения 4) использование модели yolo11s"
      ],
      "metadata": {
        "id": "yyUaOu0DSrS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d. Обучить модели с улучшенным бейзлайном на выбранном наборе данных"
      ],
      "metadata": {
        "id": "oQSG8mJ6EL6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo = YOLO('yolo11s.pt')\n",
        "res_yolo = yolo.train(\n",
        "    data=yaml_path,\n",
        "    lr0=0.01,\n",
        "    epochs=10,\n",
        "    imgsz=224,\n",
        "    batch=16,\n",
        "    augment=True,\n",
        "    name='license-plate'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU5TuhjETWWB",
        "outputId": "2bbeef54-7d25-4af1-8c45-7cec20cdc231"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=archive/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=license-plate13, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/license-plate13, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
            " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
            " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 23        [16, 19, 22]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "YOLO11s summary: 181 layers, 9,428,179 parameters, 9,428,163 gradients, 21.5 GFLOPs\n",
            "\n",
            "Transferred 493/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 753.4±380.8 MB/s, size: 24.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/archive/labels/train.cache... 1526 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1526/1526 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/archive/images/train/car-wbs-MH03AR5549_00000.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 419.6±155.5 MB/s, size: 162.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/archive/labels/val.cache... 169 images, 0 backgrounds, 0 corrupt: 100%|██████████| 169/169 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/license-plate13/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/license-plate13\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10      4.34G      1.337      1.684      1.085          6        224: 100%|██████████| 96/96 [00:16<00:00,  5.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  4.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.905      0.903      0.944      0.637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10      4.38G      1.214     0.8161      1.028          6        224: 100%|██████████| 96/96 [00:14<00:00,  6.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169     0.0427      0.467     0.0338     0.0213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10      4.42G      1.164     0.7401      1.014          6        224: 100%|██████████| 96/96 [00:14<00:00,  6.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  5.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169     0.0177      0.456     0.0121    0.00756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10      4.46G      1.107     0.6757      1.002          6        224: 100%|██████████| 96/96 [00:14<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.107      0.834      0.105     0.0702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10      4.46G      1.009     0.6145     0.9344          6        224: 100%|██████████| 96/96 [00:13<00:00,  7.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.905       0.87      0.926      0.657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10      4.46G     0.9731     0.5757     0.9306          6        224: 100%|██████████| 96/96 [00:14<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.963      0.959      0.983      0.722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10      4.46G      0.908     0.5208     0.9177          6        224: 100%|██████████| 96/96 [00:14<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.988      0.949      0.983      0.727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10      4.46G     0.8515     0.4988     0.8976          6        224: 100%|██████████| 96/96 [00:13<00:00,  6.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.988      0.953      0.987      0.755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10      4.46G     0.7989       0.45     0.8856          6        224: 100%|██████████| 96/96 [00:14<00:00,  6.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  5.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.986       0.97      0.991      0.789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10      4.46G     0.7741     0.4343     0.8721          6        224: 100%|██████████| 96/96 [00:14<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.976      0.976      0.988      0.799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.045 hours.\n",
            "Optimizer stripped from runs/detect/license-plate13/weights/last.pt, 19.1MB\n",
            "Optimizer stripped from runs/detect/license-plate13/weights/best.pt, 19.1MB\n",
            "\n",
            "Validating runs/detect/license-plate13/weights/best.pt...\n",
            "Ultralytics 8.3.135 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        169        169      0.974       0.97      0.987      0.792\n",
            "Speed: 0.0ms preprocess, 9.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/license-plate13\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e. Оценить качество моделей с улучшенным бейзлайном по выбранным метрикам на выбранном наборе данных"
      ],
      "metadata": {
        "id": "Ff2Sho-7ELpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision =  0.974\n",
        "Recall = 0.97\n",
        "mAP50 = 0.987\n",
        "mAP50-95 = 0.792"
      ],
      "metadata": {
        "id": "lWLEdvExTb3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### f. Сравнить результаты моделей с улучшенным бейзлайном в сравнении с результатами из пункта 2"
      ],
      "metadata": {
        "id": "xlf5SipbEyXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайн\n",
        "Precision = 0.987\n",
        "Recall = 0.929\n",
        "mAP50 = 0.981\n",
        "mAP50-95 = 0.739\n",
        "\n",
        "Улучшенный бейзлайн\n",
        "Precision =  0.974\n",
        "Recall = 0.97\n",
        "mAP50 = 0.987\n",
        "mAP50-95 = 0.792"
      ],
      "metadata": {
        "id": "u23FwU47TcMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### g. Сделать выводы"
      ],
      "metadata": {
        "id": "HPymHCcpEzvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью улучшеного бейзлайна удалось совсем немного повысить качество модели"
      ],
      "metadata": {
        "id": "owVjutOLUblP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Имплементация алгоритма машинного обучения"
      ],
      "metadata": {
        "id": "jdl9FL39D5FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Самостоятельно имплементировать модель машинного обучения"
      ],
      "metadata": {
        "id": "r1BS6PfbE9qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z5Esl_LHXBYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, yaml, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
      ],
      "metadata": {
        "id": "9q7fh8pElldu"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_dir = 'archive/images/train/'\n",
        "val_img_dir = 'archive/images/val/'\n",
        "train_label_dir = 'archive/labels/train/'\n",
        "val_label_dir = 'archive/labels/val/'\n",
        "classes = ['license_plate']\n",
        "num_classes = len(classes)"
      ],
      "metadata": {
        "id": "880LgJQTlXFH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyYoloDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, img_size=224, S=7, B=1, C=20, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('jpg','jpeg','png'))]\n",
        "        self.img_size = img_size\n",
        "        self.S = S\n",
        "        self.B =B\n",
        "        self.C=C\n",
        "        self.transform = transform if transform else T.Compose([\n",
        "            T.Resize((img_size, img_size)),\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
        "        label_path = os.path.join(self.label_dir, os.path.splitext(self.img_files[idx])[0] + '.txt')\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "\n",
        "        target = torch.zeros((self.S, self.S, 5*self.B + self.C))\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            for line in open(label_path, 'r').read().splitlines():\n",
        "                cls, x_ctr, y_ctr, w, h = map(float, line.split())\n",
        "                cls = int(cls)\n",
        "                i = int(x_ctr * self.S)\n",
        "                j = int(y_ctr * self.S)\n",
        "                x_cell = x_ctr * self.S - i\n",
        "                y_cell = y_ctr * self.S - j\n",
        "                if target[j, i, 4] == 0:\n",
        "                    target[j, i, 0] = x_cell\n",
        "                    target[j, i, 1] = y_cell\n",
        "                    target[j, i, 2] = w\n",
        "                    target[j, i, 3] = h\n",
        "                    target[j, i, 4] = 1\n",
        "                    target[j, i, 5 + cls] = 1\n",
        "                x_abs = x_ctr * self.img_size\n",
        "                y_abs = y_ctr * self.img_size\n",
        "                w_abs = w * self.img_size\n",
        "                h_abs = h * self.img_size\n",
        "                x1 = x_abs - w_abs/2\n",
        "                y1 = y_abs - h_abs/2\n",
        "                x2 = x_abs + w_abs/2\n",
        "                y2 = y_abs + h_abs/2\n",
        "                boxes.append([x1, y1, x2, y2])\n",
        "                labels.append(cls)\n",
        "\n",
        "        boxes  = torch.tensor(boxes, dtype=torch.float32) if boxes  else torch.zeros((0,4), dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
        "        return img, target, boxes, labels"
      ],
      "metadata": {
        "id": "aVGpOAoGlp2m"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyYolo(nn.Module):\n",
        "    def __init__(self, S=7, B=1, C=20):\n",
        "        super().__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.backbon = nn.Sequential(\n",
        "            nn.Conv2d(3,16, 3, 1, 1), nn.BatchNorm2d(16), nn.LeakyReLU(0.1), nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(16, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.LeakyReLU(0.1), nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.1), nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.1), nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(128, 256,3, 1, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.1), nn.MaxPool2d(2,2),\n",
        "            )\n",
        "        self.pred = nn.Conv2d(256, self.B*5 + self.C, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =self.backbon(x)\n",
        "        x =self.pred(x)\n",
        "        x =x.permute(0, 2, 3, 1)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "iK5IBMuqlFYs"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=1, C=20, lambda_coord=5, lambda_noobj=0.5):\n",
        "        super().__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.lambda_coord = lambda_coord\n",
        "        self.lambda_noobj = lambda_noobj\n",
        "        self.mse = nn.MSELoss(reduction='sum')\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        obj_mask = target[..., 4]\n",
        "        noobj_mask = 1-obj_mask\n",
        "\n",
        "        pred_box = torch.sigmoid(preds[..., :4])\n",
        "        pred_obj = preds[..., 4]\n",
        "        pred_cls = preds[..., 5:]\n",
        "        tgt_box = target[..., :4]\n",
        "        tgt_obj = target[..., 4]\n",
        "        tgt_cls_oh = target[..., 5:]\n",
        "\n",
        "        coord_loss = self.mse(obj_mask.unsqueeze(-1) * pred_box,obj_mask.unsqueeze(-1)*tgt_box)\n",
        "        obj_loss = torch.sum(obj_mask* self.bce(pred_obj, tgt_obj))\n",
        "        noobj_loss = torch.sum(noobj_mask *self.bce(pred_obj, tgt_obj))\n",
        "\n",
        "        pred_cls_flat = pred_cls.reshape(-1, self.C)\n",
        "        tgt_cls_ids = torch.argmax(tgt_cls_oh, dim=-1).reshape(-1)\n",
        "        cls_loss_all = self.ce(pred_cls_flat, tgt_cls_ids)\n",
        "        cls_loss = torch.sum(obj_mask.reshape(-1) * cls_loss_all)\n",
        "\n",
        "        total_loss = (self.lambda_coord * coord_loss + obj_loss + self.lambda_noobj * noobj_loss + cls_loss)\n",
        "        return total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "JuRZd7SklIOI"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(loader, model, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, y_targets, _, _ in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        y_targets = y_targets.to(device)\n",
        "        preds = model(imgs)\n",
        "        loss = loss_fn(preds, y_targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(\"loss: \", total_loss / len(loader))"
      ],
      "metadata": {
        "id": "fB8DknrnmIag"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    imgs = torch.stack([item[0] for item in batch], dim=0)\n",
        "    yolo_targets = torch.stack([item[1] for item in batch], dim=0)\n",
        "    boxes_list = [item[2] for item in batch]\n",
        "    labels_list = [item[3] for item in batch]\n",
        "    return imgs, yolo_targets, boxes_list, labels_list\n",
        "\n",
        "batch_size = 16\n",
        "img_size = 224\n",
        "train_dataset = MyYoloDataset(train_img_dir, train_label_dir, 224, 7, 1, 1)\n",
        "val_dataset = MyYoloDataset(val_img_dir, val_label_dir,  224, 7, 1, 1)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "XoOpYLaTmdie"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Обучить имплементированную модель на выбранном наборе данных"
      ],
      "metadata": {
        "id": "0VGb-oj8E9Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyYolo(7, 1, 1).to(device)\n",
        "loss_fn = YoloLoss(7, 1, 1).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f\"Epoch [{epoch}/{epochs}]\")\n",
        "    train_epoch(train_loader, model, optimizer,loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d0b086-0a64-4f1e-d578-0f7b08acf243",
        "id": "2hHoqTIUVZj7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]\n",
            "loss:  82.29254643122356\n",
            "Epoch [2/5]\n",
            "loss:  40.52777428428332\n",
            "Epoch [3/5]\n",
            "loss:  30.293502748012543\n",
            "Epoch [4/5]\n",
            "loss:  24.111383110284805\n",
            "Epoch [5/5]\n",
            "loss:  19.846983442703884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Оценить качество имплементированной модели по выбранным метрикам на выбранном наборе данных"
      ],
      "metadata": {
        "id": "CWwK3XRaE9GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "map50_metric = MeanAveragePrecision(iou_thresholds=[0.5])\n",
        "map50_95_metric = MeanAveragePrecision()\n",
        "with torch.no_grad():\n",
        "    for imgs, _, boxes_l, labels_l in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        preds = model(imgs).cpu()\n",
        "\n",
        "        for b in range(imgs.size(0)):\n",
        "            det_boxes = []\n",
        "            det_scores = []\n",
        "            det_labels = []\n",
        "            for i in range(S):\n",
        "                for j in range(S):\n",
        "                    conf = torch.sigmoid(preds[b,i,j,4])\n",
        "                    if conf < 0.5:\n",
        "                        continue\n",
        "                    x_cell, y_cell, w_cell, h_cell = torch.sigmoid(preds[b, i, j, :4])\n",
        "                    x_center = (j + x_cell)/S\n",
        "                    y_center = (i + y_cell)/S\n",
        "                    w = w_cell\n",
        "                    h = h_cell\n",
        "\n",
        "                    x1 = (x_center-w/2) * img_size\n",
        "                    y1 = (y_center-h/2) * img_size\n",
        "                    x2 = (x_center + w/2) * img_size\n",
        "                    y2 = (y_center + h/2) * img_size\n",
        "                    det_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "                    cls_probs = torch.softmax(preds[b, i, j, 5:], dim=-1)\n",
        "                    cls_score, cls_idx = torch.max(cls_probs, dim=-1)\n",
        "                    det_scores.append((conf * cls_score).item())\n",
        "                    det_labels.append(cls_idx.item())\n",
        "\n",
        "            if len(det_boxes) > 0:\n",
        "                batch_pred = {\n",
        "                    'boxes': torch.tensor(det_boxes, dtype=torch.float32),\n",
        "                    'scores': torch.tensor(det_scores, dtype=torch.float32),\n",
        "                    'labels': torch.tensor(det_labels, dtype=torch.int64)\n",
        "                }\n",
        "            else:\n",
        "                batch_pred = {\n",
        "                    'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
        "                    'scores': torch.zeros((0,), dtype=torch.float32),\n",
        "                    'labels': torch.zeros((0,), dtype=torch.int64)\n",
        "                }\n",
        "\n",
        "            gt = {\n",
        "                'boxes': boxes_l[b].to(torch.float32),\n",
        "                'labels': labels_l[b]\n",
        "            }\n",
        "            map50_metric.update([batch_pred], [gt])\n",
        "            map50_95_metric.update([batch_pred], [gt])\n",
        "\n",
        "res50 = map50_metric.compute()\n",
        "res50_95 = map50_95_metric.compute()\n",
        "\n",
        "print(f\"mAP50: {res50['map_50'].item():.4f}\")\n",
        "print(f\"mAP50-95: {res50_95['map'].item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGMeRoFwol3-",
        "outputId": "1b9e6581-bb23-42c5-b4f4-ee6c1fdd037b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mAP50: 0.2008\n",
            "mAP50-95: 0.0540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d. Сравнить результаты имплементированных моделей в сравнении с результатами из пункта 2"
      ],
      "metadata": {
        "id": "4iEXCTUaE79M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Бейзлайн\n",
        "mAP50 = 0.981\n",
        "mAP50-95 = 0.739\n",
        "\n",
        "Собственный бейзлайн\n",
        "mAP50: 0.2008\n",
        "mAP50-95: 0.0540"
      ],
      "metadata": {
        "id": "XOu8VA4TUpkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e. Сделать выводы"
      ],
      "metadata": {
        "id": "icTO5JxhE750"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собственная реализация показывает результаты значительно хуже"
      ],
      "metadata": {
        "id": "A_vK0aLAVcgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### f. Добавить техники из улучшенного бейзлайна (пункт 3с)"
      ],
      "metadata": {
        "id": "0r91YJ4qE7lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Увеличим количество эпох"
      ],
      "metadata": {
        "id": "fruilGzFV_fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "img_size = 224\n",
        "train_dataset = MyYoloDataset(train_img_dir, train_label_dir, 224, 7, 1, 1)\n",
        "val_dataset = MyYoloDataset(val_img_dir, val_label_dir,  224, 7, 1, 1)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "8nuc19jorFQ_"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### g. Обучить модели для выбранных наборов данных"
      ],
      "metadata": {
        "id": "ZhH6AytfE7Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyYolo(7, 1, 1).to(device)\n",
        "loss_fn = YoloLoss(7, 1, 1).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f\"Epoch [{epoch}/{epochs}]\")\n",
        "    train_epoch(train_loader, model, optimizer,loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83K68C5frMgH",
        "outputId": "8a3b8252-7693-491a-a6b7-ba44756e8db1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n",
            "loss:  90.38649453719457\n",
            "Epoch [2/10]\n",
            "loss:  44.86075961589813\n",
            "Epoch [3/10]\n",
            "loss:  33.08886554837227\n",
            "Epoch [4/10]\n",
            "loss:  26.19371524453163\n",
            "Epoch [5/10]\n",
            "loss:  21.67073021332423\n",
            "Epoch [6/10]\n",
            "loss:  18.313269793987274\n",
            "Epoch [7/10]\n",
            "loss:  15.374690492947897\n",
            "Epoch [8/10]\n",
            "loss:  13.160887469847998\n",
            "Epoch [9/10]\n",
            "loss:  11.133304019769033\n",
            "Epoch [10/10]\n",
            "loss:  9.440500766038895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### h. Оценить качество моделей по выбранным метрикам на выбранном наборе данных"
      ],
      "metadata": {
        "id": "wW3M0dOLE6y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "map50_metric = MeanAveragePrecision(iou_thresholds=[0.5])\n",
        "map50_95_metric = MeanAveragePrecision()\n",
        "with torch.no_grad():\n",
        "    for imgs, _, boxes_l, labels_l in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        preds = model(imgs).cpu()\n",
        "\n",
        "        for b in range(imgs.size(0)):\n",
        "            det_boxes = []\n",
        "            det_scores = []\n",
        "            det_labels = []\n",
        "            for i in range(S):\n",
        "                for j in range(S):\n",
        "                    conf = torch.sigmoid(preds[b,i,j,4])\n",
        "                    if conf < 0.5:\n",
        "                        continue\n",
        "                    x_cell, y_cell, w_cell, h_cell = torch.sigmoid(preds[b, i, j, :4])\n",
        "                    x_center = (j + x_cell)/S\n",
        "                    y_center = (i + y_cell)/S\n",
        "                    w = w_cell\n",
        "                    h = h_cell\n",
        "\n",
        "                    x1 = (x_center-w/2) * img_size\n",
        "                    y1 = (y_center-h/2) * img_size\n",
        "                    x2 = (x_center + w/2) * img_size\n",
        "                    y2 = (y_center + h/2) * img_size\n",
        "                    det_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "                    cls_probs = torch.softmax(preds[b, i, j, 5:], dim=-1)\n",
        "                    cls_score, cls_idx = torch.max(cls_probs, dim=-1)\n",
        "                    det_scores.append((conf * cls_score).item())\n",
        "                    det_labels.append(cls_idx.item())\n",
        "\n",
        "            if len(det_boxes) > 0:\n",
        "                batch_pred = {\n",
        "                    'boxes': torch.tensor(det_boxes, dtype=torch.float32),\n",
        "                    'scores': torch.tensor(det_scores, dtype=torch.float32),\n",
        "                    'labels': torch.tensor(det_labels, dtype=torch.int64)\n",
        "                }\n",
        "            else:\n",
        "                batch_pred = {\n",
        "                    'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
        "                    'scores': torch.zeros((0,), dtype=torch.float32),\n",
        "                    'labels': torch.zeros((0,), dtype=torch.int64)\n",
        "                }\n",
        "\n",
        "            gt = {\n",
        "                'boxes': boxes_l[b].to(torch.float32),\n",
        "                'labels': labels_l[b]\n",
        "            }\n",
        "            map50_metric.update([batch_pred], [gt])\n",
        "            map50_95_metric.update([batch_pred], [gt])\n",
        "\n",
        "res50 = map50_metric.compute()\n",
        "res50_95 = map50_95_metric.compute()\n",
        "\n",
        "print(f\"mAP50: {res50['map_50'].item():.4f}\")\n",
        "print(f\"mAP50-95: {res50_95['map'].item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_OTxSm8sOgq",
        "outputId": "e5a63218-db16-499a-9301-72b6ef4dcc20"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mAP50: 0.3018\n",
            "mAP50-95: 0.0793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i. Сравнить результаты моделей в сравнении с результатами из пункта 3\n"
      ],
      "metadata": {
        "id": "YWldrJSNFQXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Улучшенный бейзлайн\n",
        "mAP50 = 0.987\n",
        "mAP50-95 = 0.792\n",
        "\n",
        "\n",
        "Собственный улучшенный бейзлайн\n",
        "mAP50: 0.3018\n",
        "mAP50-95: 0.0793"
      ],
      "metadata": {
        "id": "uHCP3FF9UtB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  j. Сделать выводы"
      ],
      "metadata": {
        "id": "J-q5-l-3FWme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собственная реализация показывает результаты значительно хуже, так как архитектура  Yolo довольно сложна, увеличение эпох немного улучшило качество собственной модели"
      ],
      "metadata": {
        "id": "tGsc0CxaV7bR"
      }
    }
  ]
}